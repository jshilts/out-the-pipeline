
https://www.science.org/content/blog-post/non-reproducible-science-survey
# Non-Reproducible Science: A Survey (July 2013)

## 1. SUMMARY
This 2013 blog commentary discusses a newly published PLOS ONE survey conducted among faculty and trainees at MD Anderson Cancer Center regarding data reproducibility in academic science. The survey found that approximately 50% of respondents had experienced at least one episode where they could not reproduce published data. Many who attempted to address these issues with original authors received negative or indifferent responses. The article identifies the pressure to publish, particularly among trainees, as a major contributing factor—nearly one-third felt pressure to prove a mentor's hypothesis even when data did not support it. Additionally, the author notes complications arising from the significant proportion of visiting scientists from authority-centered cultures and the reliance on trainee positions for visa status. The commentary calls for addressing systemic issues including publication rewards, publishing practices, and statistical rigor. It ends by welcoming the Reproducibility Initiative's grant program for validating experimental psychology studies—a field grappling with reproducibility crises—while expressing the hope for similar initiatives in biomedical research.

## 2. HISTORY
After the article's publication in 2013, the reproducibility crisis continued to gain significant attention across scientific fields, particularly in psychology and biomedicine. The Reproducibility Initiative evolved and expanded, contributing to broader efforts like the Center for Open Science founded in 2013 by Brian Nosek. A landmark 2015 study in *Science* by the Open Science Collaboration attempted to replicate 100 psychology studies and found that only about 36-47% replicated successfully, depending on measurement criteria. This catalyzed widespread reforms in psychology including pre-registration, larger sample sizes, and open data practices. In cancer biology, a 2012 *Nature* study found that only 11% of preclinical cancer research could be validated, and subsequent efforts by organizations like the Reproducibility Project: Cancer Biology (launched 2013) revealed similarly stark findings—ultimately leading to systematic replication failures in approximately 70-90% of highly cited cancer studies attempted. The FDA has increasingly emphasized rigorous preclinical validation in drug submissions, correlating with heightened scrutiny over cell line authentication and experimental design. MD Anderson Cancer Center, where the original survey was conducted, implemented enhanced rigor and reproducibility training programs for trainees, while the NIH introduced new grant review criteria emphasizing reproducibility. The biopharmaceutical industry further adapted by increasing internal validation of academic findings before target selection, with many companies citing high academic irreproducibility as justification for internal target discovery programs.

## 3. PREDICTIONS
• **"Some of our publishing practices...are the equivalent of habitually walking away with the doors unlocked and the keys in the ignition. Rewarding academic scientists so directly for the number of their publications is one of the big ones."**  
This prediction identified a systemic issue that persisted but also saw reform: many institutions began reducing emphasis on publication count, adopting metrics like the San Francisco Declaration on Research Assessment (DORA, 2012) that de-emphasize journal impact factors and shift toward evaluating research quality and reproducibility.

• **"This effort [Reproducibility Initiative] is concentrating on experimental psychology...but I'll be glad to see some of this done over here in the biomedical field, too."**  
Systematic reproducibility initiatives expanded substantially in biomedicine: the Reproducibility Project: Cancer Biology (2013-2017), various NIH programs, and meta-analyses revealing 70-90% irreproducibility in preclinical cancer research. The biomedical field now widely acknowledges these issues with institutional reforms at major cancer centers.

• **"What we need to do is find ways to make it harder to cheat, and less rewarding - that will at least slow it down a bit."**  
Substantial institutional changes occurred: many journals now require data sharing, raw data availability, pre-registration of studies, and statistical review; funding agencies (NIH, NSF) mandate enhanced reproducibility training; and universities have increasingly adopted research integrity protocols.

## 4. INTEREST
Rating: **8/10**
This article anticipated the reproducibility crisis's full scope across biomedical sciences and articulated structural solutions that later influenced institutional reforms, making it prescient regarding both problem recognition and systemic remedies.


----
_model_params = {'model': 'nex-agi/deepseek-v3.1-nex-n1:free', 'input': '20130709-non-reproducible-science-survey.txt', 'reasoning': {'effort': 'high'}, 'text': {'verbosity': 'medium'}, 'prompt-template': 'prompt-template-2'}_