
https://www.science.org/content/blog-post/down-p-values
# Down With P Values (February 2014)

## 1. SUMMARY

The article summarizes critiques of p-value usage in scientific research, drawing from a Nature feature that highlighted widespread statistical misunderstandings. A core point was that p-values don't actually represent the probability that a result is a false alarm—a common misconception—but rather summarize data assuming a specific null hypothesis. The real probability of a finding being genuine depends on the prior odds that an effect exists in the first place, making ostensibly significant p-values potentially misleading for implausible hypotheses.

The second major critique focused on how p-values encourage confusion between statistical significance and practical relevance. The article cited a study showing online daters had lower divorce rates with impressive p-values (<0.002), yet the actual effect sizes were minimal: divorce rates only shifted from 7.67% to 5.96%, and marital satisfaction barely changed. This exemplified how researchers and readers can be seduced by "statistical significance" while ignoring whether the effect matters in practical terms. The piece expressed pessimism about prospects for reform, noting these problems had been raised repeatedly with little systemic change, suggesting the current approach was never designed for how it's actually used.

## 2. HISTORY

The 2014 critiques marked the beginning of an organized reform movement in statistical practice that gained substantial traction over the subsequent decade.

**American Statistical Association Statement (2016)**: In a historic move, the ASA issued its first-ever policy statement on statistical significance and p-values, explicitly warning that "p-values do not measure the probability that the studied hypothesis is true." The statement outlined six principles including that p-values cannot determine whether a hypothesis is true or important, and that by themselves p-values don't provide good evidence for a model or hypothesis. This gave institutional weight to concerns previously seen as outsider critiques.

**Journal Policy Changes (2017-2020)**: Multiple high-profile journals modified their statistical standards. *Basic and Applied Social Psychology* banned p-values entirely in 2015, though this remained an outlier. More impactful, *Nature Human Behaviour* announced in 2017 that it would require effect sizes and confidence intervals, not just p-values. *Psychological Science* introduced the "new statistics" movement emphasizing effect sizes. The real policy shift came in 2019 when the ASA published further guidance recommending researchers abandon dichotomous "significant/non-significant" thinking.

**Replication Crisis Intensification**: The period 2015-2020 saw highly public replication failures across psychology, cancer biology, and preclinical research. The Reproducibility Project: Psychology (2015) found only 36% of replications achieved statistically significant results, even when original studies had p<0.05. These failures were often attributed to p-value misuse—p-hacking, selective reporting, and misunderstanding what p-values actually measure. Meta-research studies showed p-values were among the most misunderstood concepts in scientific literature.

**Methodological Shifts**: By 2020-2023, evidence synthesis approaches gained prominence. The FDA issued guidance in 2021 encouraging Bayesian methods alongside traditional approaches. Pharmaceutical companies increasingly adopted Bayesian adaptive trial designs. However, uptake varied dramatically by field—biomedical research showed gradual change while psychology and social sciences moved faster, though p-values remained deeply entrenched in many disciplines' workflows and training programs.

**Concrete Business Impact**: The statistical reform debate affected startup ventures focused on scientific software, with platforms like JASP (free software emphasizing Bayesian analysis) gaining institutional adoptions, and companies offering reproducibility-focused tools receiving funding. However, major statistical software providers (SPSS, SAS) maintained p-value centric defaults through the period, suggesting market inertia.

## 3. PREDICTIONS

• **Prediction**: The article expressed pessimism about the likelihood of systemic change, noting these problems had been raised many times before "to little or no effect."

**Reality**: This prediction was partially correct about the persistence of the problem but underestimated the organized reform movement that emerged. While p-values remained widely used through 2024, the period saw significant institutional responses including ASA statements, journal policies, and methodological shifts toward Bayesian methods and effect sizes. Change was slower and more uneven than reform advocates hoped, but more substantial than the 2014 pessimism suggested.

• **Implied Prediction**: The article suggested researchers should focus on "How much of an effect is there?" rather than "Is there an effect?"

**Reality**: This recommendation gained substantial traction, particularly in psychology and biomedical research journals that increasingly required effect sizes and confidence intervals. Systematic reviews and meta-analyses commonly emphasized magnitude of effects over statistical significance alone. However, dichotomous thinking remained prevalent in grant review and publication decisions, suggesting the cultural shift was incomplete.

• **Implied Prediction**: Better statistical communication methods would be needed for scientists.

**Reality**: This materialized through new guidelines, educational resources, and statistical packages emphasizing interpretation over dichotomous decisions. Major journals adopted reporting standards requiring effect sizes. However, many graduate programs retained traditional p-value focused curricula, limiting widespread adoption of alternative approaches.

## 4. INTEREST

Rating: **7/10**

This article proved prescient in highlighting statistical issues that would drive major methodological reforms and replication crisis discussions over the subsequent decade. While focused on a technical statistical topic, it connected to broader questions about scientific credibility and evidence quality that became increasingly prominent in public and scientific discourse.


----
_model_params = {'model': 'nex-agi/deepseek-v3.1-nex-n1:free', 'input': '20140217-down-p-values.txt', 'reasoning': {'effort': 'high'}, 'text': {'verbosity': 'medium'}, 'prompt-template': 'prompt-template-2'}_