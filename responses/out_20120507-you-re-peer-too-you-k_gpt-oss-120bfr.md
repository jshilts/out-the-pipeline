
https://www.science.org/content/blog-post/you-re-peer-too-you-know
# You're A Peer, Too, You Know (May 2012)

## 1. SUMMARY  
The piece is a short opinion column that riffs on a New York Times editorial about the “long tail” of online comments that now accompany any high‑profile article. The author, Jack Hitt, argues that the true, final version of a scientific paper is no longer the static PDF that appears in a journal but the living document that is continuously annotated, critiqued, and debated by anyone who can read it. He suggests that the comment stream should be curated, linked to the original article, and treated as an essential part of scientific discourse—essentially expanding the notion of “peer review” to include every informed reader.

## 2. HISTORY  
**Post‑2012 growth of post‑publication commentary**  

| Development | What happened | Impact |
|---|---|---|
| **PubPeer (launched 2012)** | An anonymous‑or‑named platform for commenting on published papers. By 2024 it hosted millions of comments, many of which led to corrections, retractions, or additional experiments. | Demonstrated that a centralized comment hub can surface errors quickly; several high‑profile retractions (e.g., papers on stem‑cell pluripotency, certain cancer biomarkers) were triggered by PubPeer threads. |
| **Journal‑hosted comment sections** | Major publishers (Elsevier, Wiley, Springer Nature) experimented with embedded comment tools (e.g., *eLife*’s “eLife Lens”). Adoption was uneven; many journals disabled comments by the late 2010s due to moderation burden and low engagement. | Showed that native comment sections are not a universal solution; most community discussion migrated to external platforms. |
| **Preprint servers and open review** | bioRxiv (launched 2013) and medRxiv (2019) added “comment” and “review” buttons. By 2022, over 200 000 preprints had received at least one public comment. Platforms such as **Review Commons** (2020) and **eLife’s Open Peer Review** (2021) offered transparent reviewer reports linked to the manuscript. | Shifted a large fraction of early‑stage critique to the preprint stage, reducing the reliance on post‑publication journal comments. |
| **Social‑media‑driven discussion** | Twitter (now X) and Reddit (r/science, r/biology) became de‑facto venues for rapid paper commentary. Hashtag threads (e.g., #ScienceTwitter) often surface within hours of a paper’s release. | Provided a fast, informal layer of “crowd‑review” but with limited archiving; some journals now embed tweet‑streams in article pages. |
| **Policy and cultural changes** | Funding agencies (e.g., NIH, European Commission) began requiring data and code availability, indirectly encouraging community scrutiny. Several journals (e.g., *PNAS*, *Science*) introduced “registered reports” and “transparent peer review” where reviewer identities and reports are published alongside the article. | Institutionalized aspects of open critique, aligning with Hitt’s vision of broader participation. |
| **Retraction Watch & Crossref Event Data** | Services that track corrections and retractions have become standard references for the community. | Provide a curated record of post‑publication outcomes, complementing comment threads. |

Overall, the ecosystem has indeed moved toward a model where the “final” scientific record includes a mixture of formal peer review, post‑publication comments, and community‑driven validation. However, the comment stream is now fragmented across multiple platforms rather than being a single curated appendix to each article.

## 3. PREDICTIONS  
The article implied several future trends. Below are the most explicit ones and how they fared:

- **Prediction:** *Comments will be formally linked to the original article and curated like the article itself.*  
  **Outcome:** Partially realized. Some journals (e.g., *eLife*, *F1000Research*) publish reviewer reports and author responses alongside the paper. PubPeer and other third‑party sites provide linked comment threads, but there is no universal curation standard across all publishers.

- **Prediction:** *The “peer” in peer review will expand to “everyone who can read and understand the paper.”*  
  **Outcome:** Largely true in practice for preprints and social‑media discussion. Nonetheless, formal credit for post‑publication review remains limited; most academic incentives still focus on traditional peer review.

- **Prediction (implicit):** *The print version will become a “trophy” copy, while the online, annotated version will be the definitive record.*  
  **Outcome:** Accurate for most high‑impact journals that have shifted to online‑first publishing. Print issues have continued to decline, and many societies now issue digital PDFs with DOI‑linked comment histories.

- **Prediction:** *Noise will be an acceptable price for faster, more complete scientific self‑correction.*  
  **Outcome:** Confirmed. The volume of low‑quality or off‑topic comments is high, but community moderation tools (up‑voting, flagging) and reputation systems (e.g., on PubPeer) have mitigated the worst effects.

## 4. INTEREST  
Rating: **7/10**  
The article is a prescient snapshot of the early conversation about open, post‑publication discourse; it anticipated many real changes, though it over‑estimated the uniformity of implementation. Its relevance endures for anyone studying the evolution of scientific communication.


----
_model_params = {'model': 'openai/gpt-oss-120b:free', 'input': '20120507-you-re-peer-too-you-know.txt', 'reasoning': {'effort': 'high'}, 'text': {'verbosity': 'medium'}, 'prompt-template': 'prompt-template-2'}_