
https://www.science.org/content/blog-post/measure-measure
# "Measure for Measure" (November 2002)

## 1. SUMMARY

This opinion piece critiques attempts to create performance metrics and rankings for scientific researchers within organizations. The author argues that research work is fundamentally difficult to quantify because of its unpredictable nature, with long periods of apparent failure followed by sudden breakthroughs. The article highlights numerous paradoxes in evaluation: Should credit go to the idea originator or the person who implements it? How do you evaluate someone who makes a discovery accidentally versus systematically? How do you compare someone who produces many mediocre compounds versus someone who produces few but highly active ones? The author contends that any numerical ranking system inevitably fails because research outcomes depend heavily on luck, timing, and external factors beyond individual control, rendering traditional performance metrics inadequate.

## 2. HISTORY

In the two decades since this article's publication, the challenges of measuring scientific research performance have persisted and arguably intensified across academia, biotech, and pharmaceutical industries. Several trends emerged:

**Performance metrics evolution**: Organizations increasingly adopted bibliometric measures (h-index, citation counts, journal impact factors) and grant funding totals as proxies for research quality, especially in academia. However, these measures faced growing criticism for encouraging quantity over quality and gaming behaviors.

**Clinical development realities**: In drug discovery, the fundamental feast-or-famine dynamic described in 2002 remains largely unchanged. Clinical trial failure rates for new compounds still exceed 90%, making it difficult to judge research quality by outcomes alone. Companies that bet heavily on perceived "star" researchers or predictive metrics often experienced high-profile failures when promising early-stage programs collapsed in later-phase trials.

**Industry practices**: Pharmaceutical and biotech companies continued struggling with research performance evaluation, though many shifted toward portfolio-level metrics rather than individual rankings. Portfolio diversification became a key strategy to manage the innate unpredictability the article describes.

**Emerging challenges**: New complications arose with interdisciplinary research, where contributions became even harder to parse among team members, and with computational approaches, where credit allocation between experimentalists and data scientists proved problematic.

## 3. PREDICTIONS

• **Implicit prediction**: Numerical ranking systems for researchers would continue failing due to research's inherent unpredictability
  - **Outcome**: Largely validated. While organizations still attempt various metrics, widespread recognition exists that these measures are imperfect and often counterproductive. The reproducibility crisis and ongoing debates about research assessment reform underscore continued dissatisfaction with current approaches.

• **Implicit prediction**: The tension between rewarding process versus outcomes would remain unresolved
  - **Outcome**: Validated. This tension persists across research organizations, with no consensus solution. Different institutions emphasize different approaches, but most acknowledge both process and outcomes matter while struggling to appropriately weight them.

## 4. INTEREST

Rating: **5/10**

This article addresses a persistent, real problem in research management but lacks the specificity of case studies or novel insights that would elevate its long-term importance. The observations were astute but not particularly prescient or unique, representing common frustrations in research organizations that remain relevant but haven't driven significant systemic change.

The piece's enduring value lies in articulating fundamental tensions rather than offering solutions or capturing a critical historical moment, making it moderately interesting but not exceptional.


----
_model_params = {'model': 'nex-agi/deepseek-v3.1-nex-n1:free', 'input': '20021107-measure-measure.txt', 'reasoning': {'effort': 'high'}, 'text': {'verbosity': 'medium'}, 'prompt-template': 'prompt-template-2'}_