
https://www.science.org/content/blog-post/pipeline-10525
# Thoughts on Reproducibility (September 2015)

## 1. SUMMARY

This commentary discusses the Reproducibility Initiative's findings on attempted replications of 98 experimental psychology studies from high-tier journals. The replication results were concerning: while 97% of original studies had statistically significant results, only 36% of replications did. The mean effect size of replications was roughly half that of original studies (r = 0.197 vs. 0.403), and only 39% of effects were subjectively rated as having replicated.

The author argues this doesn't necessarily invalidate the original studies but reveals they weren't as robust as believed. Key factors include over-reliance on p-values (especially results just crossing the 0.05 threshold), underpowered studies, unconscious researcher bias, and "P-hacking." The author notes these replication attempts were conducted under unusually favorable conditions (top journals, author cooperation, careful methodology), suggesting the problem may be even worse in less ideal circumstances.

The proposed solutions include requiring better statistical rigor and properly powered studies, but the author acknowledges this would reduce publication quantity—what they call "Better Will Mean Less"—creating tension with academic incentive structures that reward high publication counts.

## 2. HISTORY

The reproducibility crisis discussion sparked by this 2015 article catalyzed significant methodological reforms across multiple scientific fields over the subsequent decade:

**Psychology**: The field implemented widespread reforms including pre-registration of studies, open data sharing requirements, larger sample sizes, and abandoning p-value worship. Many journals now require power analyses and effect sizes. The field moved toward meta-analyses and systematic replications as gold standards.

**Biomedical Research**: Pharma companies (including Amgen and Bayer) publicly acknowledged around 2011-2012 that they could only reproduce roughly 10-25% of published preclinical studies. This led to cross-industry initiatives like the Pistoia Alliance for data standards and increased internal validation requirements before pursuing drug targets from academic literature.

**Cancer Biology**: High-profile failures to reproduce key findings (including some from top-tier journals) led to systematic replication efforts. The Reproducibility Project: Cancer Biology launched in 2013-2014 found substantial replication challenges, influencing how pharmaceutical companies evaluate academic research for drug development pipelines.

**Clinical Trials**: Enhanced registration requirements (ClinicalTrials.gov mandates expanded), protocol pre-registration became standard, and journal requirements for clinical trial reporting became more stringent. Negative results gained more publication opportunities through journals dedicated to null findings.

**Statistical Reform**: Major journals (including Nature, Science, PNAS) revised statistical reporting requirements. The American Statistical Association issued its first-ever statement on p-values in 2016, emphasizing they don't measure effect size or importance. Bayesian methods gained broader acceptance as alternatives or supplements to null hypothesis significance testing.

**Business Impact**: Companies built around research reproducibility emerged. The Reproducibility Initiative evolved into the Center for Open Science, which developed infrastructure (OSF.io) used by thousands of researchers. Academic hiring criteria began slowly shifting from pure publication counts toward quality metrics and replication records, though progress has been gradual.

## 3. PREDICTIONS

**Prediction 1**: "Better Will Mean Less" - Stricter reproducibility standards would reduce publication quantity
- **Outcome**: **Partially accurate but nuanced**. While many fields did implement stricter standards (pre-registration, larger samples, open data), publication volume has continued increasing overall. However, the growth rate may have moderated in specific subfields, and the "less" manifested more in reduced numbers of underpowered studies rather than overall output decline. The prediction correctly identified the tension between quality and quantity incentives.

**Prediction 2**: Statistical rigor reforms would face resistance because "too many people need and want to publish papers" and are "used to the statistical regimes we already have"
- **Outcome**: **Accurate**. Despite strong evidence and advocacy, widespread p-value misuse persists. Many researchers continue questionable research practices. Change has been slower than reformers hoped, particularly outside top-tier institutions and journals. The culture shift remains incomplete across scientific fields.

**Prediction 3**: The reported ~40% replication rate was "probably a boundary; it's just going to get worse from there"
- **Outcome**: **Accurate**. Subsequent replication efforts in other fields confirmed even lower rates. Cancer biology replication attempts showed similar or worse results. The most favorable interpretation is that psychology's upper-tier journals actually represent the higher end of reproducibility, with other fields and lower-tier venues performing worse.

**Prediction 4**: Suggestions for improvement (better powered studies, understanding limits of statistical significance) would face implementation challenges
- **Outcome**: **Accurate**. While methodological improvements occurred, resistance persists. Power analyses are often performed perfunctorily. The p-value debate continues with strong opinions on both sides. Pre-registration adoption remains incomplete. The predicted cultural inertia indeed slowed reform implementation.

## 4. INTEREST

**Rating: 9/10**

This article captures a pivotal moment in scientific self-examination that reshaped research practices across multiple disciplines. Its accurate predictions about the persistence of statistical problems and its clear articulation of incentive misalignments demonstrate remarkable prescience about a crisis whose full dimensions were still emerging.


----
_model_params = {'model': 'nex-agi/deepseek-v3.1-nex-n1:free', 'input': '20150902-pipeline-10525.txt', 'reasoning': {'effort': 'high'}, 'text': {'verbosity': 'medium'}, 'prompt-template': 'prompt-template-2'}_