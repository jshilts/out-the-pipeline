
https://www.science.org/content/blog-post/they-can-be-ranked-somehow
# They Can Be Ranked - Somehow (January 2008)

## 1. SUMMARY

This article discusses journal impact factors and their reliability issues, sparked by a critical paper from Rockefeller University Press. The piece highlights problems with Thomson's methodology for calculating impact factors, including inconsistent counting of what qualifies as "citable articles" versus "front matter," and Thomson's inability (or unwillingness) to provide the raw data used for their calculations—a standard that would be unacceptable for regular scientific publications.

The author also examines alternative journal ranking systems that were emerging, particularly one that ranked chemistry journals. While acknowledging that review journals tend to dominate such lists, the article focuses on the lower-ranked journals, identifying obscure publications from Slovakia (Chemical Papers), Bulgaria (Oxidation Communications), and other developing regions, as well as noting the poor performance of some established journals like Beilstein Journal of Organic Chemistry.

## 2. HISTORY

**Impact Factor Controversy and Reform:**
The problematic issues with Thomson Reuters' impact factors identified in the article became widely recognized over the following decade. In 2016, the Declaration on Research Assessment (DORA) was signed by numerous institutions, explicitly calling for an end to impact factor use in hiring, promotion, and funding decisions. Major funding agencies and institutions began adopting alternative metrics.

**Open Access and Alternative Metrics:**
The period after 2008 saw explosive growth in open access publishing, with new metrics like Article-Level Metrics (ALM), Altmetric scores, and PLUM Analytics emerging. Publishers began providing article-level usage statistics, addressing some of the granularity problems the article highlighted.

**Journals Mentioned - Fate:**
- **Beilstein Journal of Organic Chemistry** (ranked 468/470): Ceased publication in 2018, validating the article's assessment of its poor performance
- Many of the obscure journals mentioned (Bulgarian, Egyptian, Ethiopian chemistry journals) largely remained obscure and continued to have minimal impact
- The alternative ranking system mentioned never gained widespread adoption compared to Google's h-index and other emerging metrics

**Meta-Analyses Validation:**
Multiple studies published after 2008 confirmed that impact factors correlate poorly with individual article citation rates, supporting the Rockefeller University Press's criticisms. The scientific community increasingly recognized that journal-level metrics are poor proxies for individual research quality.

## 3. PREDICTIONS

The article made the following implicit and explicit predictions:

• **Implicit: Impact factors would face increased scrutiny** → **Accurate**: The 2016 DORA declaration and widespread institutional reforms showed this happened

• **Implicit: Alternative ranking systems would emerge** → **Mixed**: While alternatives emerged, no single system replaced impact factors completely; instead a multiplicity of metrics developed

• **Explicit: Beilstein Journal of Organic Chemistry would fail** → **Accurate**: The journal ceased publication in 2018, just ten years later

• **Implicit: Data transparency issues would be resolved** → **Partially Accurate**: While Thomson improved transparency somewhat, the fundamental methodology remained opaque to users

## 4. INTEREST

Rating: **8/10**

This article demonstrates remarkable prescience about the coming crisis in research assessment and predates major reform movements by eight years. Its identification of specific problems and prediction of journal failures make it a historically significant piece of science policy commentary.


----
_model_params = {'model': 'nex-agi/deepseek-v3.1-nex-n1:free', 'input': '20080111-they-can-be-ranked-somehow.txt', 'reasoning': {'effort': 'high'}, 'text': {'verbosity': 'medium'}, 'prompt-template': 'prompt-template-2'}_