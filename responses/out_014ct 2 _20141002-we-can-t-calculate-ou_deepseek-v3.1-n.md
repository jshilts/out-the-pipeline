model_params = {'model': 'nex-agi/deepseek-v3.1-nex-n1:free', 'input': '20141002-we-can-t-calculate-our-way-out-one.txt', 'reasoning': {'effort': 'high'}, 'text': {'verbosity': 'medium'}}

# Retrospective Analysis: "We Can't Calculate Our Way Out of This One" (2014)

## 1. SUMMARY

The article critiques the widespread practice in drug discovery of using simple physicochemical property filters—particularly the "Pfizer rule" (clogP >3 and TPSA <75 Å²)—to predict toxicity and reduce clinical trial failures. The author highlights a 2013 MedChemComm study analyzing 130 AstraZeneca development compounds that found these supposedly predictive rules were not only invalid but actually inverted in their dataset. The study also examined approved drugs from 2009-2012 and found that applying such filters would have eliminated many successful medications, particularly those falling into the high-clogP/high-TPSA category that the rules were meant to flag as problematic. The article's central thesis is that the pharmaceutical industry was placing undue faith in oversimplified metrics that lacked universal validity.

## 2. HISTORY

The skepticism expressed in this 2014 article proved remarkably prescient. In the subsequent decade, the pharmaceutical industry witnessed several related developments:

**The AI/ML Revolution (2015-2020)**: The period immediately following this article saw an explosion in machine learning approaches to drug discovery. Companies like Atomwise, Exscientia, and DeepMind's AlphaFold promised to solve exactly the problems the article highlighted. However, the initial hype exceeded the practical impact—while these approaches showed promise in retrospective validation, their real-world success remained limited.

**Continued High Failure Rates**: Between 2014-2024, clinical trial failure rates remained stubbornly high, hovering around 90% for drugs entering clinical trials. This didn't fundamentally improve despite massive investments in computational approaches and AI.

**The Rise of Precision Medicine**: The industry increasingly recognized that context-dependent factors (therapeutic area, patient population, genetic variations) mattered far more than simple physicochemical properties, echoing the article's critique of universal rules.

**Emerging Complex Modalities**: The 2014-2024 period saw the rise of biologics, gene therapies, and cell therapies—modalities where traditional "drug-likeness" rules were completely irrelevant, further highlighting the limitations of simple molecular property approaches.

**Big Data Disillusionment**: By the early 2020s, many in the industry began recognizing that simply throwing more data at machine learning models didn't automatically solve fundamental biological complexity.

## 3. PREDICTIONS

**What the Article Got Right:**
- **The fundamental critique**: The article correctly predicted that simple physicochemical rules lacked universal validity and that the industry was over-relying on them.
- **The direction of travel**: It correctly anticipated that the field needed to move away from overly simplistic filters toward more nuanced, context-aware approaches.
- **The complexity problem**: The emphasis on biological complexity and the inadequacy of simple metrics has held up extremely well.

**What the Article Underestimated:**
- **The scale of the AI hype cycle**: While the article correctly questioned simple metrics, it probably didn't foresee how intensely the industry would subsequently invest in complex AI/ML approaches, many of which ended up having similar validation problems.
- **Regulatory impact**: The article didn't fully anticipate how regulatory agencies (FDA, EMA) would grapple with AI/ML in drug development, initially encouraging their use but later becoming more cautious about model validation.
- **The persistence of the problem**: While the article was skeptical about simple solutions, the continued high failure rates through 2024 suggest that even with better understanding, the fundamental challenges remain.

**What Wasn't Mentioned But Became Important:**
- The article didn't discuss the emerging importance of real-world evidence, patient-reported outcomes, or the role of healthcare economics in drug development decisions—all of which became increasingly significant factors.

## 4. INTEREST

**Score: 6**

This article deserves a solid 6 out of 9 for several reasons. While it didn't predict major technological breakthroughs or industry transformations, it articulated an important cautionary message that proved prescient. The pharmaceutical industry's continued struggles with drug development failures, despite massive investments in computational approaches, validates the article's skeptical stance. The core insight—that biological systems are too complex for simple reductionist approaches—has only become more relevant as we've discovered additional layers of complexity in disease biology.

The article also gains points for representing a broader philosophical stance that remains important: the tension between computational approaches and empirical reality in drug discovery. However, it loses some points because it presented primarily a critique rather than constructive alternatives (though the author did acknowledge this limitation).

The timing also matters—published in 2014, it came just before the peak hype cycle for AI in drug discovery, making it somewhat ahead of its time in questioning whether computational approaches would solve fundamental problems. Overall, it's a thoughtful piece that accurately identified real limitations in the field's approach, even if it couldn't offer clear solutions.