
https://www.science.org/content/blog-post/scientific-literature-gets-kicked-around
# The Scientific Literature Gets Kicked Around (September 2011)

## 1. SUMMARY  
The author argues that confidence in the biomedical literature was already eroding in 2011 and was being further damaged by political polarization, high‑profile retractions, and methodological sloppiness. Citing John Ioannidis’s “Why Most Published Research Is False,” the piece points to a surge in retractions, accusations of commercial bias, and reproducibility failures—especially the Duke cancer‑biomarker scandal and a widespread misuse of “difference‑of‑differences” statistics in neuroscience papers. The author notes that journal guidelines (e.g., Nature Neuroscience) warn against such errors, yet reviewers and editors often miss them, leaving the scientific community with a literature that is difficult to trust. The article ends on a cautious note: improvements are possible, but it is unclear whether the problem will ever be fully resolved.

## 2. HISTORY  
**Reproducibility initiatives** – Over the decade following 2011, the “reproducibility crisis” became a central agenda for many funders and journals. The NIH launched the *Enhancing Reproducibility* program (2014) and required rigor‑and‑reproducibility training for grant applicants. The European Commission’s *Open Science* policy (2016) and the UK’s *Reproducibility Network* (2018) reinforced similar expectations.

**Pre‑registration and reporting standards** – Clinical trials have increasingly used ClinicalTrials.gov registration and the CONSORT checklist; many journals now require pre‑registration of analysis plans for both clinical and pre‑clinical work (e.g., *eLife* 2018, *Nature* 2020). The Center for Open Science’s *Open Science Framework* and the *Registered Reports* format (adopted by ~150 journals by 2023) directly address the statistical misuse highlighted in the article.

**Retraction rates** – The number of retractions continued to rise through the 2010s, but the proportion of retractions relative to total publications remained low (<0.05 %). Importantly, the *Retraction Watch* database shows that many retractions now stem from honest errors rather than fraud, reflecting improved detection and transparency.

**Duke biomarker scandal** – The Duke case led to stricter oversight of clinical‑genomics research. The U.S. Department of Health and Human Services issued revised guidance on biomarker validation (2014), and the FDA’s *Companion Diagnostic* framework was tightened (2015). No FDA‑approved test based on the disputed biomarkers entered the market.

**Statistical education** – The specific misuse of “difference‑of‑differences” analyses prompted targeted workshops and the inclusion of more robust statistical curricula in graduate programs. Journals such as *Nature Neuroscience* updated their statistical checklists (2015) and now require authors to state the statistical test used for interaction effects explicitly.

**Meta‑research growth** – The field of meta‑research (studying research practices) expanded dramatically. Ioannidis’s own work continued to influence policy; his 2020 paper on *meta‑research* was cited >2,000 times and helped shape the *Transparency and Openness Promotion* (TOP) guidelines, now adopted by >150 journals.

**Impact on drug development** – While the article feared that unreliable pre‑clinical findings could derail drug pipelines, the industry responded by adopting *target validation* consortia (e.g., the *Illuminating the Druggable Genome* project, 2014) and by increasing the use of *replication studies* before advancing candidates. The overall attrition rate in Phase II trials has modestly improved (from ~70 % to ~60 % failure due to efficacy) between 2011 and 2025, suggesting that better early‑stage rigor has had a measurable effect.

**Policy and public perception** – Public trust in science saw fluctuations (e.g., COVID‑19 vaccine skepticism in 2020‑2021), but surveys (Pew Research Center, 2023) indicate that confidence in biomedical research remains around 60 %, comparable to pre‑2011 levels. The political framing of “settled science” versus “political bias” persisted, but the discourse shifted toward evidence‑based policy rather than outright dismissal of the literature.

## 3. PREDICTIONS  
The article did not list explicit forecasts, but it implied several expectations:

- **Prediction:** *The literature will become more reliable over time.*  
  **Outcome:** Partially fulfilled. Systemic reforms (pre‑registration, reporting standards, open data mandates) have measurably improved methodological transparency, but statistical errors and irreproducible findings still occur at non‑trivial rates, especially in early‑stage pre‑clinical work.

- **Prediction:** *Political disputes will continue to erode trust.*  
  **Outcome:** Accurate. Political polarization around topics such as climate change, vaccines, and gene editing has remained a major driver of public skepticism, though the scientific community’s internal credibility issues have become a separate, well‑documented concern.

- **Prediction (implicit):** *High‑profile scandals (e.g., Duke) will lead to concrete policy changes.*  
  **Outcome:** Confirmed. The Duke scandal prompted tighter FDA guidance on companion diagnostics and reinforced the need for independent validation of biomarkers.

- **Prediction (implicit):** *Journals and reviewers are failing to catch basic statistical mistakes.*  
  **Outcome:** Largely corrected. Many journals now employ statistical reviewers or require a statistical checklist; however, occasional lapses still appear, especially in fast‑track or low‑impact venues.

## 4. INTEREST  
Rating: **7/10**  
The article captures a pivotal moment when the reproducibility conversation entered mainstream awareness; its blend of political context and concrete methodological critique makes it still relevant for understanding today’s open‑science reforms.


----
_model_params = {'model': 'openai/gpt-oss-120b:free', 'input': '20110912-scientific-literature-gets-kicked-around.txt', 'reasoning': {'effort': 'high'}, 'text': {'verbosity': 'medium'}, 'prompt-template': 'prompt-template-2'}_