
https://www.science.org/content/blog-post/sloppy-science
# Sloppy Science (Mar 2012)

## 1. SUMMARY  
The commentary by Derek Lowe (published 29 Mar 2012) highlighted a Nature “Comment” by Glenn Begley and Lee Ellis that exposed a striking lack of reproducibility in pre‑clinical oncology research.  Begley and Ellis reported that, when Amgen’s oncology group attempted to replicate 53 “landmark” cancer‑biology papers, only 6 (≈11 %) could be reproduced even after extensive communication with the original authors and use of additional models.  The article argued that many studies suffered from poor experimental design—unblinded assays, selective reporting of favorable results, and insufficient controls—rather than outright fraud.  It used the high‑profile failure of Sanofi’s PARP inhibitor iniparib (Phase III flop) and the contemporaneous troubles of AstraZeneca’s olaparib as concrete illustrations of how shaky pre‑clinical data can derail drug development.  The piece concluded with a call for tighter standards (blinded protocols, full data disclosure) in pre‑clinical work.

## 2. HISTORY  
**Reproducibility initiatives (2012‑present)**  
- **Amgen’s internal audit** was followed by similar “re‑validation” projects at other pharma (e.g., Novartis, Bayer, Pfizer) that reported reproducibility rates ranging from 10‑30 % for oncology targets.  
- **The Reproducibility Initiative** (launched 2013) and the **Center for Open Science** (2014) created platforms for independent replication of high‑impact studies.  
- **Journals** introduced checklists and reporting standards: Nature’s “Transparent Reporting” (2013), Cell’s “STAR Methods” (2016), and the **ARRIVE** and **MDAR** guidelines for animal and cell‑based work.  
- **Funding agencies** responded: the NIH instituted the “Rigor and Reproducibility” policy (2016) requiring detailed methodological descriptions, authentication of cell lines, and statistical power calculations in grant applications. The European Commission’s Horizon 2020 program added similar criteria.  

**Impact on drug development**  
- **Iniparib**: After the 2011 Phase III failure, Sanofi discontinued its development; the compound never reached the market. Subsequent analyses confirmed that iniparib is a weak, non‑specific agent rather than a true PARP1 inhibitor.  
- **Olaparib**: Contrary to the article’s implication that PARP inhibition might be a dead‑end, olaparib (Lynparza) received FDA approval in 2014 for BRCA‑mutated ovarian cancer and later for breast, pancreatic, and prostate cancers. Its success helped cement PARP inhibition as a validated therapeutic strategy, though the field learned to demand more rigorous pre‑clinical validation (e.g., synthetic lethality assays, orthogonal models).  
- **Broader oncology pipeline**: The overall conversion rate of pre‑clinical oncology hits to approved drugs has remained low (~5 % of candidates entering IND). However, the industry now routinely incorporates **blinded in‑vivo studies**, **independent replication**, and **cell‑line authentication** as standard operating procedures, partly driven by the 2012 controversy.  

**Cultural shift**  
- The “replication crisis” entered mainstream scientific discourse, leading to the 2015 **“Reproducibility and Replicability in Science”** report by the National Academies and the 2018 **“Science of Science”** initiatives at the NIH.  
- Pre‑print servers (bioRxiv, medRxiv) and open‑data mandates have increased transparency, allowing other groups to spot inconsistencies earlier.  
- Nonetheless, surveys in 2020‑2023 still report that a sizable fraction of researchers perceive reproducibility problems as “moderate to severe,” indicating that the issue raised in 2012 persists despite progress.

## 3. PREDICTIONS  

| Prediction mentioned or implied in the article | What actually happened |
|---|---|
| **Most pre‑clinical oncology papers are not reproducible (≈90 % failure)** | Independent audits (Amgen, Novartis, Bayer) confirmed low reproducibility (10‑30 % success) for selected “high‑impact” studies, aligning with the article’s claim. |
| **Iniparib would fail in late‑stage trials** | Correct: Phase III trial (2011) showed no survival benefit; development was halted. |
| **PARP inhibitors as a class might be ineffective** | Partially wrong: While iniparib failed, olaparib and later talazoparib, rucaparib, and niraparib achieved multiple FDA approvals and are now standard of care for BRCA‑mutated and HR‑deficient cancers. |
| **Stricter pre‑clinical standards (blinding, full data reporting) would be adopted** | Largely true: Major journals and funding agencies introduced mandatory methodological checklists, cell‑line authentication, and statistical power statements. Adoption is uneven but has become a norm in many high‑profile labs. |
| **The reproducibility crisis would spur industry‑wide reforms** | Accurate: Pharma now routinely performs internal replication before advancing projects, and many companies have formal “reproducibility” teams. |
| **The problem would be limited to oncology** | Incorrect: Similar reproducibility concerns have been documented across neuroscience, immunology, and microbiology, leading to cross‑disciplinary reforms. |

## 4. INTEREST  
**Rating: 8/10** – The article captured a pivotal moment that sparked a lasting, industry‑wide reassessment of pre‑clinical rigor; its concrete examples (iniparib, olaparib) and the Amgen audit make it a landmark commentary on the reproducibility crisis.


----
_model_params = {'model': 'openai/gpt-oss-120b:free', 'input': '20120329-sloppy-science.txt', 'reasoning': {'effort': 'high'}, 'text': {'verbosity': 'medium'}, 'prompt-template': 'prompt-template-2'}_